{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOL9SA5Y6lxN"
   },
   "source": [
    "# Big Data Analytic Project CA-2 Repeat\n",
    " by Muhammad Haseeb Sheikh 2023120 CA-2\n",
    "\n",
    " > In this project, I have explored twitter data. Then I perform sentiment analysis on each tweet, and predict its emotion by using natural language processing techniques. In this pipeline, I first analysed the polarity and subjectivity of each tweet, and then predict the emotion in six metrics: happy, bad, encouraged, joy, loving, and depressed. Besides of these, I also build a binary classifier to find out if a tweet is insult speech. There are five hundred thousands tweets per day approximately and 9,841,743 in total (until the submission), to properly handle them, we indeed build a cluster with four nodes to process the tweets everyday. In this notebook, I have provided the visualization result which are also available online at https://covid19.yaonotes.org. There's also a short introduction on why we want to make this site (on the right upper corner). This notebook was tested with Google Colab. All data are avaliable at Google Drive:https://drive.google.com/drive/folders/1g1ZqcZ3xRtDKrR3aIl1bLs2vzetCa0g0?usp=sharing. Codes for the system and website can also be accessed at Github: https://github.com/xzyaoi/covid-sentiment.\n",
    "\n",
    "<!-- ![ChessUrl](https://raw.githubusercontent.com/xzyaoi/covid-sentiment/master/ezgif.com-gif-maker.gif \"full-visualizer\") -->\n",
    "\n",
    "\n",
    "This notebook will be organized into the following parts:\n",
    "\n",
    "* **Dependencies Installation**. In this code block, we will declare all our dependencies and provide code to install them.\n",
    "\n",
    "* **Data Exploration.** We will play around with the sample data we have crawled in step 2 and demonstrate what is included in the dataset.\n",
    "\n",
    "* **Build Sentiment Analyser.** In this part, we will do three things. </br>1) we will use an existing library, the TextBlob, to predict the polarity and subjectivity. </br>2) we will use a neural network to estimate the emotions of a text. </br>3) we will use another neural network to detect insult speech.\n",
    "\n",
    "* **Sentiment Analysis.** In this part, we will demonstrate how we actually perform the prediction on our dataset. We use the apply function in pandas, and make it parallel to improve the speed. We also draw the wordcloud as background.\n",
    "\n",
    "* **Data Collection and Merging.** We then collect the result, and resample the granularity to minute-level.\n",
    "\n",
    "* **Data Visualization.** In this part, we will  visualize the result into a line chart to demonstrate the changes in people's emotion during the pandemic.\n",
    "\n",
    "* **Data Warehouse.** We will talk about how we store the data in our data warehouse.\n",
    "\n",
    "* **Conclusion and Discussion.** In this part, we will discuss the results that we have in the visualization and exploration part. We will conclude how people's emotion change during the time period.\n",
    "\n",
    "* **Implementation, Limitations and Possible Improvements.** We will introduce how we actually implement this in our cluster, the limitations and possible improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L-Zlcv6gEW2T"
   },
   "outputs": [],
   "source": [
    "## Files needed\n",
    "# !wget https://github.com/aidmodels/sentiment-analysis/releases/download/v0.1/model.h5 -O /content/drive/MyDrive/ColabNotebooks/CCT/sentiment.h5\n",
    "# !wget https://raw.githubusercontent.com/aidmodels/sentiment-analysis/master/pretrained/tokenizer.pickle -O /content/drive/MyDrive/ColabNotebooks/CCT/tokenizer.pickle\n",
    "# !wget https://raw.githubusercontent.com/deepmipt/DeepPavlov/0.10.0/deeppavlov/configs/classifiers/insults_kaggle_conv_bert.json -P /content/drive/MyDrive/ColabNotebooks/CCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2f3Tkbh9_Hpg"
   },
   "outputs": [],
   "source": [
    "## Change links in configuration file to github address, this is because the original server is too slow.\n",
    "!gsed -i 's,http://files.deeppavlov.ai/datasets/insults_data.tar.gz,https://github.com/aidmodels/insult_detection/releases/download/v0.1/insults_data.tar.gz,g' ./insults_kaggle_conv_bert.json\n",
    "!gsed -i 's,http://files.deeppavlov.ai/deeppavlov_data/bert/conversational_cased_L-12_H-768_A-12.tar.gz,https://github.com/aidmodels/insult_detection/releases/download/v0.1/conversational_cased_L-12_H-768_A-12.tar.gz,g' ./insults_kaggle_conv_bert.json\n",
    "!gsed -i 's,http://files.deeppavlov.ai/deeppavlov_data/classifiers/insults_kaggle_v4.tar.gz,https://github.com/aidmodels/insult_detection/releases/download/v0.1/insults_kaggle_v4.tar.gz,g' ./insults_kaggle_conv_bert.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1WxDCCnp5Xu"
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "In this section, we will perform some basic data exploration about the data we have. Current, we are not able to do much, as we only have two key properties that we are interested: the retweets_count and likes_count.\n",
    "\n",
    "After we get the sentiment data, we will perform a deeper exploration. We will have more important metrics over there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SLOiQvpSOzWT",
    "outputId": "2ddb409b-81a3-4772-abe3-9236a178ccee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1249857397839073291</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>00:29:59</td>\n",
       "      <td>Could Coronavirus Trigger Force Majeure Contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1249857397771862026</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>00:29:59</td>\n",
       "      <td>[Get the Infographic] With the COVID-19 situat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1249857397721575429</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>00:29:59</td>\n",
       "      <td>IMF approves $500m in debt relief for 25 count...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1249857397700481025</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>00:29:59</td>\n",
       "      <td>@JeffBezos is our modem Louis XVI. Richer than...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1249857397427974144</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>00:29:59</td>\n",
       "      <td>Dr. Michael Wilkes breaks down why nursing hom...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id        date      time  \\\n",
       "0  1249857397839073291  2020-04-14  00:29:59   \n",
       "1  1249857397771862026  2020-04-14  00:29:59   \n",
       "2  1249857397721575429  2020-04-14  00:29:59   \n",
       "3  1249857397700481025  2020-04-14  00:29:59   \n",
       "4  1249857397427974144  2020-04-14  00:29:59   \n",
       "\n",
       "                                               tweet  retweets_count  \\\n",
       "0  Could Coronavirus Trigger Force Majeure Contra...               0   \n",
       "1  [Get the Infographic] With the COVID-19 situat...               0   \n",
       "2  IMF approves $500m in debt relief for 25 count...               0   \n",
       "3  @JeffBezos is our modem Louis XVI. Richer than...               0   \n",
       "4  Dr. Michael Wilkes breaks down why nursing hom...               2   \n",
       "\n",
       "   likes_count  \n",
       "0            0  \n",
       "1            0  \n",
       "2            1  \n",
       "3            0  \n",
       "4            6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you do not want to crawling the data again, use this code block to download a sample.\n",
    "## It is the data crawled in 2020-04-14 00:00:00 to 2020-04-14 00:30:00\n",
    "import pandas as pd\n",
    "df_sample = pd.read_csv('https://raw.githubusercontent.com/CConstance/tweets_sentiment/master/test.csv')\n",
    "display(df_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path = '/Users/macbook/Work/CCT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bVgltPR4dc8C",
    "outputId": "c7062ae9-5102-473f-9df9-8693740af047"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>raw_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                          time  \\\n",
       "0  1467810369  Mon Apr 06 22:19:45 PDT 2009   \n",
       "1  1467810672  Mon Apr 06 22:19:49 PDT 2009   \n",
       "2  1467810917  Mon Apr 06 22:19:53 PDT 2009   \n",
       "3  1467811184  Mon Apr 06 22:19:57 PDT 2009   \n",
       "4  1467811193  Mon Apr 06 22:19:57 PDT 2009   \n",
       "\n",
       "                                           raw_tweet  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Exploration and cleaning\n",
    "# Take a look at origin crawling data\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df = pd.read_csv(os.path.join(abs_path, 'ProjectTweets.csv'),\n",
    "                 names = ['id', 'time', 'query', \n",
    "                          'username', 'raw_tweet']).drop(columns=['query','username'])\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ySCCkeqtvn7o"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "def convert_to_utc(input_str):\n",
    "    if 'PDT' in input_str:\n",
    "        input_str = input_str.replace('PDT', 'UTC')\n",
    "    # Define the input format based on the provided example\n",
    "    input_format = \"%a %b %d %H:%M:%S %Z %Y\"\n",
    "\n",
    "    # Parse the input string into a datetime object\n",
    "    input_datetime = datetime.strptime(input_str, input_format)\n",
    "\n",
    "    # Replace the timezone information with UTC\n",
    "    input_datetime_utc = input_datetime.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # Format the output as per your requirement\n",
    "    output_time = input_datetime_utc.strftime(\"%H:%M:%S\")\n",
    "    output_date = input_datetime_utc.strftime(\"%Y-%m-%d\")\n",
    "#     output_day = input_datetime_utc.strftime(\"%A\")\n",
    "\n",
    "    return output_time, output_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JDms5K4U5M5y"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "df[['output_time', 'output_date']] = df['time'].apply(convert_to_utc).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PtJq6ZIm5R1s",
    "outputId": "082e435d-1f4e-441c-a83c-f06a2aae2de9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>raw_tweet</th>\n",
       "      <th>output_time</th>\n",
       "      <th>output_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>2009-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>2009-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>2009-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>2009-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>2009-04-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                          time  \\\n",
       "0  1467810369  Mon Apr 06 22:19:45 PDT 2009   \n",
       "1  1467810672  Mon Apr 06 22:19:49 PDT 2009   \n",
       "2  1467810917  Mon Apr 06 22:19:53 PDT 2009   \n",
       "3  1467811184  Mon Apr 06 22:19:57 PDT 2009   \n",
       "4  1467811193  Mon Apr 06 22:19:57 PDT 2009   \n",
       "\n",
       "                                           raw_tweet output_time output_date  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...    22:19:45  2009-04-06  \n",
       "1  is upset that he can't update his Facebook by ...    22:19:49  2009-04-06  \n",
       "2  @Kenichan I dived many times for the ball. Man...    22:19:53  2009-04-06  \n",
       "3    my whole body feels itchy and like its on fire     22:19:57  2009-04-06  \n",
       "4  @nationwideclass no, it's not behaving at all....    22:19:57  2009-04-06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "S-5zG54e-JuS",
    "outputId": "1405303b-c684-4ebb-86a4-24751abfb630"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>raw_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810369</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>2193601966</td>\n",
       "      <td>2009-06-16</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>2193601969</td>\n",
       "      <td>2009-06-16</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>2193601991</td>\n",
       "      <td>2009-06-16</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>2193602064</td>\n",
       "      <td>2009-06-16</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>2193602129</td>\n",
       "      <td>2009-06-16</td>\n",
       "      <td>08:40:50</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id        date      time  \\\n",
       "0        1467810369  2009-04-06  22:19:45   \n",
       "1        1467810672  2009-04-06  22:19:49   \n",
       "2        1467810917  2009-04-06  22:19:53   \n",
       "3        1467811184  2009-04-06  22:19:57   \n",
       "4        1467811193  2009-04-06  22:19:57   \n",
       "...             ...         ...       ...   \n",
       "1599995  2193601966  2009-06-16  08:40:49   \n",
       "1599996  2193601969  2009-06-16  08:40:49   \n",
       "1599997  2193601991  2009-06-16  08:40:49   \n",
       "1599998  2193602064  2009-06-16  08:40:49   \n",
       "1599999  2193602129  2009-06-16  08:40:50   \n",
       "\n",
       "                                                 raw_tweet  \n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1        is upset that he can't update his Facebook by ...  \n",
       "2        @Kenichan I dived many times for the ball. Man...  \n",
       "3          my whole body feels itchy and like its on fire   \n",
       "4        @nationwideclass no, it's not behaving at all....  \n",
       "...                                                    ...  \n",
       "1599995  Just woke up. Having no school is the best fee...  \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.drop(columns=['time'])\n",
    "df = df.rename(columns={'output_time': 'time', 'output_date': 'date'})\n",
    "\n",
    "# Rearrange columns\n",
    "column_order = ['id', 'date', 'time', 'raw_tweet']\n",
    "df = df[column_order]\n",
    "# df = df.drop(columns=['day'])\n",
    "# Print the resulting DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyCvqfVGP2iK"
   },
   "source": [
    "In original crawling data, they have 34 colunms of items. During our sentiment analysis, we want to analyse the emotion from tweets for a period of time. Thus, we need to crawl tweets every day. It has several hundreds of thousand of tweets per day. To reduce the amount of fetched data, we filtered irrelvant items and left id, tweets and time while fetching data. ID is used to track the source of tweet. The content of tweet is used to analyse the emotion of users. Time records the changes of emotion.\n",
    "\n",
    "Even though most tweets do not get any responses, it is still worth investigating them as they are reflecting the feelings and emotions of people at that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjinNvyCpvm3"
   },
   "source": [
    "## Build Sentiment Analyser\n",
    "\n",
    "In this section, we will build and test three sentiment analysers. They are of different metrics:\n",
    "\n",
    "* **Emotion Recognition.** We have build a neural-network-based emotion extractor before, and will use the algorithm to extract emotion information from tweets. It will give 6 emotions: bad, depressed, encouraged, happy, joy, sad, loving. The output of every emotion is a real value ranged in $[0,1]$. The neural network architecture and corresponding training code will be provided as appendix.\n",
    "\n",
    "* **Polarity and Subjectivity.** We will use [TextBlob](https://textblob.readthedocs.io/en/dev/) to analyse the polarity and subjectivity of a single sentence. It is a prebuilt library that uses NaiveBayes classifier to classify the input sentences.\n",
    "\n",
    "* **Insultation.** We will use [DeepPavlov](http://docs.deeppavlov.ai/en/master/features/models/classifiers.html) to find out if a sentence includes insult content. It is also a neural-network based text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "YXT03QpHQmy7",
    "outputId": "12d7c3d7-42bc-4135-d193-a240ca7dbfce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "{'output': [[0.0009565531508997083, 0.9974063038825989, 9.468352146768666e-08, 1.0750953833849053e-06, 8.217542131205846e-07, 0.002043356653302908, 4.878527306573233e-06]]}\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "{'output': [[0.2471114844083786, 0.22646766901016235, 0.03660554811358452, 0.05048996955156326, 0.060417741537094116, 0.2546912133693695, 0.040342219173908234]]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We use an existing neural network to perform sentiment analysis, \n",
    "in this notebook, we will present how it works.\n",
    "Inside our system, we deployed the neural network into a http server, \n",
    "and perform http requests to get the prediction.\n",
    "\"\"\"\n",
    "\n",
    "from mlpm.solver import Solver\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "MAX_LEN=140\n",
    "\n",
    "class SentimentSolver(Solver):\n",
    "    def __init__(self, toml_file=None):\n",
    "        super().__init__(toml_file)\n",
    "        # Do you Init Work here\n",
    "        with open(os.path.join(abs_path, 'tokenizer.pickle'), 'rb') as handle:\n",
    "            self.loaded_tokenizer = pickle.load(handle)\n",
    "        self.model = tf.keras.models.load_model(os.path.join(abs_path,\"sentiment.h5\"))\n",
    "        self.ready()\n",
    "\n",
    "    def infer(self, data):\n",
    "        # if you need to get file uploaded, get the path from input_file_path in data\n",
    "        sequences = self.loaded_tokenizer.texts_to_sequences([data['text']])\n",
    "        padding = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "        result = self.model.predict(padding, batch_size=1, verbose=1)\n",
    "        return {\"output\": result.tolist()} # return a dict\n",
    "\n",
    "ss = SentimentSolver()\n",
    "print(ss.infer({'text':\"I'm depressed to hear that...!\"}))\n",
    "print(ss.infer({'text':\"I felt sorrow...!\"}))\n",
    "## The output are ordered as: bad, depressed, encouraged, happy, joy, sad, loving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ku9Ubq95p_zH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.6000000000000001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "def tb_analyse(sentence):\n",
    "    tb = TextBlob(str(sentence))\n",
    "    return tb.sentiment.polarity, tb.sentiment.subjectivity\n",
    "tb_analyse(\"I am good to go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9O_M9fK2VGeF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 15:01:06.470 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from https://github.com/aidmodels/insult_detection/releases/download/v0.1/insults_kaggle_v4.tar.gz to /Users/macbook/.deeppavlov/models/insults_kaggle_v4.tar.gz\n",
      "100%|█████████████████████████████████████████████████████| 401M/401M [06:13<00:00, 1.07MB/s]\n",
      "2023-12-27 15:07:21.663 INFO in 'deeppavlov.core.data.utils'['utils'] at line 276: Extracting /Users/macbook/.deeppavlov/models/insults_kaggle_v4.tar.gz archive into /Users/macbook/.deeppavlov/models/classifiers\n",
      "2023-12-27 15:07:25.51 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from https://github.com/aidmodels/insult_detection/releases/download/v0.1/insults_data.tar.gz to /Users/macbook/.deeppavlov/insults_data.tar.gz\n",
      "100%|█████████████████████████████████████████████████████| 682k/682k [00:00<00:00, 1.48MB/s]\n",
      "2023-12-27 15:07:26.707 INFO in 'deeppavlov.core.data.utils'['utils'] at line 276: Extracting /Users/macbook/.deeppavlov/insults_data.tar.gz archive into /Users/macbook/.deeppavlov/downloads\n",
      "2023-12-27 15:07:27.199 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from https://github.com/aidmodels/insult_detection/releases/download/v0.1/conversational_cased_L-12_H-768_A-12.tar.gz to /Users/macbook/.deeppavlov/downloads/conversational_cased_L-12_H-768_A-12.tar.gz\n",
      "100%|█████████████████████████████████████████████████████| 408M/408M [05:58<00:00, 1.14MB/s]\n",
      "2023-12-27 15:13:27.138 INFO in 'deeppavlov.core.data.utils'['utils'] at line 276: Extracting /Users/macbook/.deeppavlov/downloads/conversational_cased_L-12_H-768_A-12.tar.gz archive into /Users/macbook/.deeppavlov/downloads/bert_models\n"
     ]
    },
    {
     "ename": "ConfigError",
     "evalue": "'Model bert_preprocessor is not registered.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Insult Detection\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m### It will download required models from\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeeppavlov\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./insults_kaggle_conv_bert.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(model([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHi, how are you?\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(model([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou asshole!\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/cct/lib/python3.9/site-packages/deeppavlov/core/commands/infer.py:55\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, install, download)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m parameter for the \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m component, so \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m will not be renewed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m, component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m))))\n\u001b[0;32m---> 55\u001b[0m component \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m component_config:\n\u001b[1;32m     58\u001b[0m     model\u001b[38;5;241m.\u001b[39m_components_dict[component_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m component\n",
      "File \u001b[0;32m~/anaconda3/envs/cct/lib/python3.9/site-packages/deeppavlov/core/common/params.py:92\u001b[0m, in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(e)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m---> 92\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(obj):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# find the submodels params recursively\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     config_params \u001b[38;5;241m=\u001b[39m {k: _init_param(v, mode) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config_params\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/anaconda3/envs/cct/lib/python3.9/site-packages/deeppavlov/core/common/registry.py:72\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _REGISTRY:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not registered.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cls_from_str(name)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cls_from_str(_REGISTRY[name])\n",
      "\u001b[0;31mConfigError\u001b[0m: 'Model bert_preprocessor is not registered.'"
     ]
    }
   ],
   "source": [
    "## Insult Detection\n",
    "### It will download required models from\n",
    "from deeppavlov import build_model\n",
    "model = build_model('./insults_kaggle_conv_bert.json', download=True)\n",
    "print(model(['Hi, how are you?']))\n",
    "print(model(['You asshole!']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jThmReGES7x1"
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "In this section, we will use the above analysers to analyse each tweet in our dataset. Here we will provide a function, that reads a row in the dataframe, and returns a row that includes outputs from sentiment analysis. We then want to map each row to the function, and get the output. In ```pandas```, these can be done via ```applymap (for dataframes), map (for series)``` and ```apply (for both)```. Here we use the apply function to perform the map.\n",
    "\n",
    "Though we use map to parallel our processing, it may still be a bit slow to perform such a large dataset. Therefore, we provide a post-processing ```.csv``` file for quicky explore what's inside after the sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGtKUtRTg5LP"
   },
   "outputs": [],
   "source": [
    "# Then we apply the sentiment analysis to all the texts in a pandas dataframe,\n",
    "## Unlike in Sentiment Analysis, we want to make all tweets a list, and get them all.\n",
    "## Unfortunately, this is really really slow.\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "df = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "def apply_row(row):\n",
    "  row['emotion'] = ss.infer({'text':row['tweet']})['output'][0]\n",
    "  row['is_insult'] = int(model([row['tweet']])[0] == 'Insult')\n",
    "  row['polarity'], row['subjectivity'] = tb_analyse(row['tweet'])\n",
    "  return row\n",
    "\n",
    "df = df.progress_apply(apply_row, axis = 1)\n",
    "df.to_csv(\"./test_result.csv\")\n",
    "df.head()\n",
    "total_insult = df['is_insult'].sum()\n",
    "print('Total Insult Tweets:' +str(total_insult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JWaKNyyXzjA"
   },
   "outputs": [],
   "source": [
    "# If the above code block takes too long time, consider downloading the result directly.\n",
    "!wget https://raw.githubusercontent.com/CConstance/tweets_sentiment/master/test_result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTa9j8cN6eCH"
   },
   "outputs": [],
   "source": [
    "# Demo Visualization - Data prepare\n",
    "import pandas as pd\n",
    "df_result = pd.read_csv(\"./test_result.csv\")\n",
    "\n",
    "df_result['emotion'].tolist()\n",
    "df_result['bad']=df_result['emotion'].apply(lambda x:float(x.split(\",\")[0][1:]))\n",
    "df_result['depressed']=df_result['emotion'].apply(lambda x:float(x.split(\",\")[1]))\n",
    "df_result['encouraged']=df_result['emotion'].apply(lambda x:float(x.split(\",\")[2]))\n",
    "df_result['happy']=df_result['emotion'].apply(lambda x:float(x.split(\",\")[3]))\n",
    "df_result['joy']=df_result['emotion'].apply(lambda x:float(x.split(\",\")[4]))\n",
    "df_result['loving']=df_result['emotion'].apply(lambda x:float(x.split(\",\")[5][:-1]))\n",
    "\n",
    "# Group by time\n",
    "df_result['time'] = df_result['date']+\" \"+df_result['time']\n",
    "df_result['time'] = pd.to_datetime(df_result['time'])\n",
    "\n",
    "df_result.index = df_result['time']\n",
    "df_result = df_result.drop(['emotion'], axis=1)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pmDtrY6WuFs"
   },
   "source": [
    "## Further Data Exploration\n",
    "\n",
    "In this section, we will explore the sentiment data after the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlxrBl0aZf9y"
   },
   "outputs": [],
   "source": [
    "## Summary of insult speech\n",
    "total_num = df_result['is_insult'].sum()\n",
    "print(\"There's \"+str(total_num)+\" insult speeches found in the dataset.\")\n",
    "## We found there are only 24 insult speeches. It's only a few amoung all the tweets that we have. (a bit surprising for us)\n",
    "## Let's see whats these:\n",
    "is_insult =  df_result[df_result['is_insult']==1]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(is_insult['tweet'])\n",
    "\n",
    "## We found some keywords in these tweets that make them insult others,\n",
    "## such as: 'a complete moron', 'you cold busted', 'you were incompetent', 'Blood on your hands you', 'immoral as you', 'You are disrespectful and ignorant'\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNKW6bhZW2l_"
   },
   "outputs": [],
   "source": [
    "## Relations between bad/happy, assumption: they should be negatively correlated.\n",
    "var = 'bad/happy'\n",
    "data = pd.concat([df_result['bad'], df_result['happy']], axis=1)\n",
    "data.plot.scatter(x='happy', y='bad', ylim=(0,1));\n",
    "\n",
    "## The result aligns with our assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU7GwwFSY1tx"
   },
   "outputs": [],
   "source": [
    "## Relations between polarity and happy, assumption: they should be positvely correlated.\n",
    "var = 'polarity/happy'\n",
    "data = pd.concat([df_result['polarity'], df_result['happy']], axis=1)\n",
    "data.plot.scatter(x='polarity', y='happy', ylim=(0,1));\n",
    "\n",
    "## We found that as the polarity grows, especially in $[0,1]$, there are more sentences classified as happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkzYkWDAZGQy"
   },
   "outputs": [],
   "source": [
    "## Relations between encouraged and happy, assumption: they should be positvely correlated.\n",
    "var = 'encouraged/happy'\n",
    "data = pd.concat([df_result['encouraged'], df_result['happy']], axis=1)\n",
    "data.plot.scatter(x='encouraged', y='happy', ylim=(0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcMwXsS7THj_"
   },
   "source": [
    "## Data Visualization\n",
    "\n",
    "In this section, we will visualze the emotional changes across the time. In this notebook, it is hard to demonstrate the whole dataset, and we will only illustrate the sample dataset. The same technique can be used directly for the whole dataset.\n",
    "\n",
    "We will first resample the data, by finding the mean value of different emotions, polarity and subjectivity in every minute. And then we will also find the sum of insult speeches in every minute. We can interpret the resampled data as:\n",
    "\n",
    "* Average emotions, polarity and subjectivity per minute.\n",
    "* Total insult speeches per minute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WroHu0lYOU1"
   },
   "outputs": [],
   "source": [
    "# Resample data\n",
    "## Count insult tweets by minute\n",
    "df_insult = df_result.resample('T').sum()\n",
    "df_result = df_result.resample('T').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dn3Y2FrYLtM"
   },
   "outputs": [],
   "source": [
    "df_result.plot(y=[\"polarity\", \"loving\", \"joy\", \"happy\", \"bad\", \"depressed\", \"subjectivity\", \"encouraged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0QJLWfOP36c"
   },
   "outputs": [],
   "source": [
    "# Statistics of insulat tweets per minute\n",
    "#df_insult.head()\n",
    "df_insult.plot(y=[\"is_insult\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b07o8PWvndQ"
   },
   "outputs": [],
   "source": [
    "## Wordcloud of the tweets\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "new_stopwords = ['twitter','utm_campaign','bit','bit.ly','Covid','pic','utm_source','utm_social','utm_medium','https','http','COVID','html','instagram','covid','covid19']\n",
    "stopwords.extend(new_stopwords)\n",
    "# stopwords=load_stopwords(stopwords)\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "def generate_worldcloud(texts):\n",
    "  # iterate through the csv file\n",
    "\n",
    "  wordcloud = WordCloud(width = 300, height = 300,\n",
    "                  background_color ='white',\n",
    "                  max_words=2048,\n",
    "                  stopwords=stopwords,\n",
    "                  max_font_size=25).generate(texts)\n",
    "  return wordcloud\n",
    "texts = \" \".join(str(text) for text in df_result['tweet'] if not any(x in str(text) for x in new_stopwords))\n",
    "wordcloud = generate_worldcloud(texts)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfusief0vZjp"
   },
   "source": [
    "## Data Warehousing\n",
    "\n",
    "After the sentiment analysis and generating wordcloud, our program will upload the image of that wordcloud, along with the emotion data to data warehouse, so that our frontend visualizer can read the data, draw the line graph and render the background. We store the data to two different endpoints:\n",
    "\n",
    "* **OSF(Open Science Foundation)**. They provide an easy-to-use API for quering files in the storage. We store all the wordcloud images in OSF, so that our frontend will be able to know which date is available in our dataset without loading the whole dataset. On top of that, we build a Cloud Function hosted on Google Cloud Platform to avoid cross-origin issues.\n",
    "\n",
    "* **GitHub**. We use GitHub to provide all ```.csv``` file, i.e. the emotion data per day. It will be named with the same name in the image data, and our frontend will read the data when needed. You can find all the data at https://github.com/xzyaoi/covid-sentiment/tree/master/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbNN8Z5wTM8X"
   },
   "source": [
    "## Conclusion and Discussion\n",
    "\n",
    "In this project, we crawled nearly 10 million Twitter data. We investigate how we can build our pipeline to analyse these tweets, visualize the results, store them properly. We build three analysers using different techniques, and **extract** the polarity, subjectivity, 6 emotions and if they are insulting others. We also drawn a wordcloud per day to show what people are caring about. We then **transform** the emotion data per minute to compute the average emotion metrics and number of insult tweets. After that we **load** the data into data warehouse. To conclude, we build a ETL system that analyses twitter data everyday.\n",
    "\n",
    "To our surprise, we found that people's emotion are pretty stable during the pandemic. Overall, there appear more encouraging and happy tweets in May and June, compared with April. Another finding is that the insulting tweets are only a few (in our sample, 24/5352=0.4%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ryc0naRFTQjl"
   },
   "source": [
    "## Implementation, Limitations and Possible Improvements.\n",
    "\n",
    "Aside of this notebook, we implemented the system into a 4-nodes cluster. They can be categorized into two types:\n",
    "\n",
    "* **Crawler**: We have a single node for crawling, hosted on Google Cloud Platform. We set a crontab on 3:00 AM every day to start the crawling for yesterday's data.\n",
    "\n",
    "* **Analyser**: Other nodes are hosted on a Chinese provider. We want to balance our requests to every node so that we can improve the performance. To achieve so, we made the sentiment analyser and insultation detector to be HTTP service, and then we deployed HAProxy as load balancer. By doing so, we can distribute the tweet across different nodes and parallelize the process.\n",
    "\n",
    "Even though we tried our best to parallelize the analysis part, it still takes a longer time than we expected and we cannot finish the insultation analysis on time. Thus, in the online version, we only leave the emotion and polarity analysis there. However, if possible we can purchase more nodes, and deploy the insultation detector and finish the insultation analysis.\n",
    "\n",
    "Another problem in our analysis is that we have not considered other keywords to search. It might be useful and interesting to look into other keywords, such as #China, #Trump to compare people's emotion changes.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cct",
   "language": "python",
   "name": "cct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
